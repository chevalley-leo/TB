\chapter{Programmation}
\label{chap:programmation}

La plupart des scripts étant conçus pour fonctionner dans \gls{aicaS} et donc \gls{ros2}, il a été nécessaire d'adopter une architecture spécifique pour permettre la communication entre les différents éléments du système. Toutefois, l'objectif du travail n'était pas d'approfondir la compréhension ou la maîtrise de \gls{ros2} en tant que tel.

La plupart des programmes et blocs fonctionnels ont d'abord été développés et testés en \gls{python} de manière indépendante, sans intégration directe à \gls{aicaS}. Cette méthode a permis de valider les algorithmes et le fonctionnement général dans un environnement simple et maîtrisé. Par la suite, les scripts ont été adaptés pour fonctionner dans des blocs fonctionnels \gls{aicaS}. Comme ces blocs peuvent être écrits en \gls{python}, l'intégration s'est révélée relativement simple. Ce choix d'une démarche progressive a permis de gagner du temps lors de la phase de preuve de concept et de débogage.

Dans un second temps, lorsque le développement a été finalisé, l'intégration dans l'environnement \gls{aicaS} a été réalisée. Cette étape a consisté à adapter les scripts pour qu'ils fonctionnent avec l'architecture \gls{ros2} imposée par \gls{aicaS}, en veillant à la compatibilité des communications et à l'intégration des blocs fonctionnels dans le système global.

Cette démarche a permis de gagner du temps sur le développement initial, tout en assurant une intégration robuste et conforme aux exigences de la plateforme finale.

\clearpage
\section{Ecran tactile}
L'écran tactile \gls{ED-HMI3010-101C} (ED101) est le seul élément de la maquette qui n'est pas directement lié au logiciel \gls{aicaS}. Un script \gls{python} a été développé avec la bibliothèque \texttt{tkinter} pour permettre à l'utilisateur de dessiner et d'envoyer directement le fichier \gls{dxf} correspondant au PC principal. Un Bloc fonctionnel a été créé dans \gls{aicaS} pour recevoir ce fichier \gls{dxf}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/figures/AICA_Tcp_Server.png}
    \caption{Bloc fonctionnel du serveur TCP dans AICA Studio}
    \label{fig:touchscreen_interface}
\end{figure}

\section{Graveuse laser}
Afin de pouvoir communiquer avec la graveuse laser depuis le logiciel \gls{aicaS}, un bloc fonctionnel a été développé avec \gls{ezdxf} \cite{DXFDocs} afin de :
\begin{itemize}
    \item Charger un fichier \gls{dxf}.
    \item Convertir le fichier \gls{dxf} en \gls{gcode}.
    \item Envoyer le \gls{gcode} à la graveuse laser.
    \item Gérer la synchronisation entre le bras robot et la graveuse laser.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/figures/AICA_Laser_interface.png}
    \caption{Bloc fonctionnel de la graveuse laser dans AICA Studio}
    \label{fig:laser_interface}
\end{figure}

On peut voir en haut à gauche un signal d'entrée pour le chargement du bloc fonctionnel. Ce signal est utilisé en début de programme sur tous les blocs fonctionnels pour initialiser les paramètres nécessaires. Il permet également de désactiver et d'activer le bloc pendant l'exécution du programme. En bas à gauche, le signal d'entrée \texttt{Run gcode file} est utilisé pour charger le fichier DXF à graver. Au centre à droite, le signal de sortie \texttt{Laser Finished} permet de savoir quand la graveuse à terminé de graver la pièce.

\section{Caméra}
La caméra Intel D435 est intégrée dans le système via un bloc fonctionnel personnalisé développé dans \gls{aicaS}. Ce bloc utilise les données de profondeur captées par la caméra pour détecter la position des pièces à graver.
Afin d'obtenir un retour de données rapide, le code est optimisé pour minimiser le temps de traitement et permettre une mise à jour environ toutes les 300ms (dépendant du nombre de points du nuage). Le but est d'envoyer le robot à la position de la pièce à prendre et de corriger la trajectoire toutes les 300ms.
Le programme fonctionne selon les étapes suivantes :
\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.55\textwidth}
        \centering
        \begin{tikzpicture}[node distance=1.8cm, every node/.style={draw, align=center, rounded corners, minimum height=1cm}]

            \node (load) {Téléchargement du modèle de la pièce};
            \node (init) [below of=load] {Initialisation de la caméra};
            \node (capture) [below of=init] {Capture image et profondeur};
            \node (preprocess) [below of=capture] {Prétraitement couleur};
            \node (cloud) [below of=preprocess] {Détection du nuage de points};
            \node (match) [below of=cloud] {Correspondance de points};
            \node (matrice) [below of=match] {Calcul matrice de transformation};
            \node (publish) [below of=matrice] {Publication position et orientation};
            \draw[->] (load) -- (init);
            \draw[->] (init) -- (capture);
            \draw[->] (capture) -- (preprocess);
            \draw[->] (preprocess) -- (cloud);
            \draw[->] (cloud) -- (match);
            \draw[->] (match) -- (matrice);
            \draw[->] (matrice) -- (publish);
            \draw[->, thick] (publish.east) .. controls +(right:2.5cm) and +(right:2.5cm) .. (capture.east);
        \end{tikzpicture}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \begin{subfigure}{0.8\linewidth}
            \fbox{\includegraphics[width=\linewidth]{assets/figures/Piece_model.png}}
            \caption{Modèle 3D de la pièce}
        \end{subfigure}\\[0.2cm]
        \begin{subfigure}{0.8\linewidth}
            \fbox{\includegraphics[width=\linewidth]{assets/figures/photo_origin.png}}
            \caption{Prise de la photo}
        \end{subfigure}\\[0.2cm]
        \begin{subfigure}{0.8\linewidth}
            \fbox{\includegraphics[width=\linewidth]{assets/figures/filtrage_couleur.png}}
            \caption{Filtrage par couleur}
        \end{subfigure}\\[0.2cm]
        \begin{subfigure}{0.8\linewidth}
            \fbox{\includegraphics[width=\linewidth]{assets/figures/detection_piece.png}}
            \caption{Isolement de la pièce}
        \end{subfigure}\\[0.2cm]
        \begin{subfigure}{0.8\linewidth}
            \fbox{\includegraphics[width=\linewidth]{assets/figures/superposition.png}}
            \caption{Superposition modèle/réalité}
        \end{subfigure}
    \end{minipage}
    \caption{Schéma de séquence du programme de la caméra Intel D435, avec illustrations des étapes principales}
    \label{fig:sequence_camera_illustre}
\end{figure}

\subsection{Bloc fonctionnel de la caméra}
Tout comme le bloc fonctionnel de la graveuse laser, le bloc fonctionnel de la caméra offre une certaine liberté dans les paramètres de traitement. Il est par exemple possible de choisir la couleur de la pièce à détecter, modifier le modèle ainsi que le nombre de points de ce dernier. Cette flexibilité permet d'adapter le système tant dans la forme de la pièce à détecter que dans la rapidité de traitement. D'autres réglages sont aussi disponibles.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/figures/AICA_Camera.png}
    \caption{Bloc fonctionnel de la caméra dans AICA Studio}
    \label{fig:camera_interface}
\end{figure}

Ici, les entrées sont principalement des entrées de chargement en début de programme. Concernant les sorties, le bloc \texttt{Realsense\_Camera} envoie en entrée au bloc \texttt{Piece\_Detection} le nuage de points, les couleurs correspondantes et les informations du nuage de points. Le bloc \texttt{Piece\_Detection} utilise ces données pour détecter la pièce et publier sa position et son orientation dans le repère du robot. Les autres sorties du bloc \texttt{Piece\_Detection} permettent d'activer ou désactiver les blocs d'approche ou de retour en position de base en fonction de la confirmation ou non de la bonne détection de la pièce.

\subsection{Transformation du repère caméra au repère robot}

\sloppy

Pour obtenir la position de la pièce détectée dans le repère du robot (ou du monde), le programme effectue une composition de transformations :

\subsubsection{Détection et rotation de la pièce dans le repère caméra}

La position de la pièce est estimée en superposant le centroïde du modèle 3D (référence) et celui de la pièce détectée dans l'image. La rotation est déterminée en testant différentes orientations (rotation degré par degré autour de chaque axe) pour trouver celle qui aligne au mieux le modèle 3D avec la pièce détectée.

\setlength{\belowcaptionskip}{0pt}
\begin{listing}[H]
    \inputminted{python}{assets/code/rotation.py}
    \caption{Extrait de code pour la rotation de la pièce dans le repère caméra}
\end{listing}

La méthode consiste à déterminer successivement la meilleure rotation pour chaque axe : on commence par l’axe Y, puis on ajuste l’orientation de la pièce selon cette rotation optimale. Ensuite, on procède de la même manière pour l’axe X, puis enfin pour l’axe Z. À chaque étape, le code teste différentes valeurs de rotation et sélectionne celle qui minimise l’erreur d’alignement entre le modèle et la pièce détectée.
Pour illustrer ce processus, les exemples présentés ci-dessous montrent des pièces volontairement orientées uniquement selon un seul axe à la fois (Y, puis X, puis Z). Cela permet de visualiser clairement l’effet de chaque étape et de mieux comprendre le fonctionnement de l’algorithme. Dans chaque cas et pour gagner du temps de traitement, la rotation est effectuée en incréments de 3 degrés. Le code peut être facilement modifié pour ajuster cette valeur si nécessaire.

\clearpage
\subsubsection{Rotation autour de l'axe Y}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{assets/figures/rote_y_avt_cote.png}
    \caption{Rotation autour de l’axe Y : avant rotation (vue de côté)}
    \label{fig:rot_y_avt_cote}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{assets/figures/rote_y_avt_face.png}
    \caption{Rotation autour de l’axe Y : avant rotation (vue de face)}
    \label{fig:rot_y_avt_face}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.70\textwidth]{assets/figures/rot_y_aprs_cote.png}
    \caption{Rotation autour de l’axe Y : résultat après alignement (vue de côté)}
    \label{fig:rot_y_aprs_cote}
\end{figure}
On peut voir que même avec une pièce très désaxée, le programme arrive à faire une rotation correcte et très satisfaisante. Il est à noter que la rotation est effectuée autour du centroïde de la pièce, ce qui permet de ne pas avoir de décalage dans la position de la pièce.

\clearpage

\subsubsection{Rotation autour de l'axe X}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{assets/figures/rot_x_avt_cote.png}
    \caption{Rotation autour de l’axe X : avant rotation (vue de côté)}
    \label{fig:rot_x_avt_cote}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{assets/figures/rot_x_avt_face.png}
    \caption{Rotation autour de l’axe X : avant rotation (vue de face)}
    \label{fig:rot_x_avt_face}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.70\textwidth]{assets/figures/rot_x_aprs_cote.png}
    \caption{Rotation autour de l’axe X : résultat après alignement (vue de côté)}
    \label{fig:rot_x_aprs_cote}
\end{figure}
Ici, on constate que la pièce de base est très bruitée. Cela est dû à la qualité de la caméra. Cependant, le programme arrive tout de même à faire une rotation correcte.

\clearpage

\subsubsection{Rotation autour de l'axe Z}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.50\textwidth]{assets/figures/rot_z_avt.png}
    \caption{Rotation autour de l’axe Z : avant rotation (vue de face)}
    \label{fig:rot_z_avt}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/figures/rot_z_aprs.png}
    \caption{Rotation autour de l’axe Z : résultat après alignement (vue de face)}
    \label{fig:rot_z_aprs}
\end{figure}
Dans ce cas, on peut voir que la rotation est très efficace. Même avec une pièce désaxée de plus de 90 degrés, le programme arrive à faire une rotation correcte. En pratique, le bras robot ne pouvant pas tourner autour de la pièce de plus de 30 degrés dans l'axe X et Y et de 45 degrés dans l'axe Z, il n'est pas nécessaire de faire des calculs pour une rotation aussi importante que celle montrée dans l'image ci-dessus. Il est tout de même intéressant de voir que le programme est capable de gérer des rotations importantes.
\subsubsection{Transformation du repère caméra au repère monde (robot)}

La position et l’orientation de la caméra dans le repère monde sont connues (translation et quaternion). On calcule la matrice de transformation homogène $T_{\text{cam} \to \text{world}}$.

\subsubsection{Calcul de la position finale de la pièce dans le monde}

On compose la transformation de la pièce dans la caméra ($T_{\text{piece} \to \text{cam}}$) avec celle de la caméra dans le monde ($T_{\text{cam} \to \text{world}}$) :
\[
    T_{\text{piece} \to \text{world}} = T_{\text{cam} \to \text{world}} \cdot T_{\text{piece} \to \text{cam}}
\]
On obtient ainsi la position et l’orientation de la pièce dans le repère monde (robot), ce qui permet au robot de s'y rendre.

La librairie \texttt{scipy.spatial.transform.Rotation} est utilisée pour convertir les \gls{quaternions} en matrices de rotation, et \texttt{numpy} pour manipuler les matrices de transformation.

Cette méthode permet de publier la position et l'orientation de la pièce dans le repère global, ce qui est indispensable pour que le robot puisse saisir ou interagir avec la pièce détectée.

\subsection{Valeurs de la matrice de transformation}

Dans ce projet, il n'a pas été nécessaire de réaliser une calibration manuelle entre la caméra et le repère robot. En effet, grâce au modèle 3D précis de la maquette, la position et l'orientation de la caméra ont pu être déterminées virtuellement, ce qui a permis de renseigner directement la matrice de transformation dans le programme.

\begin{minipage}{0.55\textwidth}
    \paragraph{Transformation caméra → monde}
    La pose de la caméra dans le monde utilisée dans le code est la suivante :
    \begin{itemize}
        \item Translation (m) : $[0.420451,\ 0.0175,\ 0.766251]$
        \item Quaternion (w, x, y, z) : $[0,\ -0.707107,\ 0.707107,\ 0]$
    \end{itemize}
    {\rowcolors{2}{}{}}%
    La matrice de transformation homogène correspondante est :
    \begin{equation*}
        T_{\text{cam} \to \text{world}} =
        \begin{bmatrix}
            0  & -1 & 0  & 0.420451 \\
            -1 & 0  & 0  & 0.0175   \\
            0  & 0  & -1 & 0.766251 \\
            0  & 0  & 0  & 1
        \end{bmatrix}
    \end{equation*}
    Cette transformation permet de passer des coordonnées caméra aux coordonnées monde (robot), ce qui est indispensable pour la localisation de la pièce par le robot.
\end{minipage}%
\hfill
\begin{minipage}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\linewidth]{assets/figures/Transform_example.png}
    \captionof{figure}{Illustration de la transformation de repère caméra vers monde}
\end{minipage}


\clearpage
\section{Programme AICA Studio}

Cette section présente l'intégration des différents blocs fonctionnels développés. A l'aide de blocs de base du logiciel et de certains blocs fournis après coup par l'entreprise, il a été possible de créer un programme qui relie tous les éléments de la maquette ensemble.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{assets/figures/AICA_PROG.png}
    \caption{Programme AICA Studio de la maquette de la graveuse laser intelligente}
    \label{fig:aica_programme}
\end{figure}

On constate beaucoup d'éléments différents dans le programme, notamment :

\begin{enumerate}
    \item Le bloc \gls{tcp} de communication avec le PC de l'écran tactile.
    \item Le bloc de la graveuse laser qui permet de charger le fichier \gls{dxf} et de lancer la machine
    \item Le bloc de la caméra qui permet de détecter la position de la pièce.
    \item Le bloc du bras robot qui permet de communiquer directement avec l'appareil
    \item Le bloc de séquence qui permet de gérer l'ordre d'exécution des différentes tâches lorsque le robot attrape une pièce.
    \item Les blocs de gestion de prise de pièce qui assurent une approche correcte et une préhension efficace.
    \item Les blocs de gestion de vérification de prise de pièce et d'avortement de la tâche en cours si nécessaire.
\end{enumerate}

Certains de ces blocs ou groupes de blocs ont déjà été expliqués ci-dessus, d'autres seront détaillés dans les sections suivantes.

\subsection{Bloc du bras robot}

Le bloc du bras robot permet la communication entre le programme et les différents actionneurs du robot. Plusieurs options de contrôleurs sont disponibles, permettant de piloter le robot en contrôle de vélocité ou de trajectoire (mais pas simultanément). Le bloc gère également l'ouverture et la fermeture de la pince.

\begin{figure}[H]
    \centering
    \includegraphics[height=0.8\textheight,keepaspectratio]{assets/figures/AICA_Hardware_interface.png}
    \caption{Bloc du bras robot dans AICA Studio}
    \label{fig:robot_block}
\end{figure}

On constate plusieurs entrées et sorties dans le bloc du bras robot qui se réunissent en trois types :
\begin{itemize}
    \item L'entrée d'activation ou de désactivation du bloc entier.
    \item Les entrées de commande du robot.
    \item Les sorties d'états du robot.
\end{itemize}
Dans le cas des sorties d'états du robot, toutes ne sont pas utilisées dans ce projet et donc n'apparaissent pas sur l'illustration.



\subsection{Bloc de séquence}

Le bloc de séquence est utilisé dans la seconde partie du programme. Il sert à traiter des événements tel que des déplacements ainsi que l'envoi et la réception de signaux.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{assets/figures/AICA_Sequence (2).png}
    \caption{Bloc de séquence dans AICA Studio}
    \label{fig:sequence_block}
\end{figure}

Afin de mieux comprendre le fonctionnement de ce bloc, les étapes de la séquences ont été encadrées en couleur :
\begin{itemize}
    \item \textbf{\textcolor{violet}{Violet}} : Délai pour laisser le temps aux contrôleurs de se désactiver ou de s'activer.
    \item \textbf{\textcolor{yellow!80!black}{Jaune}} : Commande d'activations ou de désactivations des contrôleurs.
    \item \textbf{\textcolor{green!70!black}{Vert}} : Commande de position du robot.
    \item \textbf{\textcolor{brown}{Brun}} : Attente de validation de la prise de la pièce.
    \item \textbf{\textcolor{red}{Rouge}} : Validation du mouvement du robot.
    \item \textbf{\textcolor{magenta}{Rose}} : Commande de la graveuse laser.
    \item \textbf{\textcolor{black}{Blanc}} : Validation de la fin de séquence de la graveuse laser.
    \item \textbf{\textcolor{cyan}{Bleu clair}} : Ouverture de la pince du robot.
\end{itemize}

\subsection{Blocs de gestion de prise de pièce}

Les blocs de gestion de prise de la pièce sont divisés en deux parties qui fonctionnent en alternance. La première partie est utilisée pour l'approche de la pièce et la seconde pour le retour dans la position de départ dans le cas ou la pièce n'est plus détectée. L'avantage de cette approche est que le robot se met automatiquement dans la position de départ au démarrage du programme.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/figures/AICA_Prise (2).png}
    \caption{Blocs de gestion de prise de pièce dans AICA Studio}
    \label{fig:piece_block}
\end{figure}


Le fonctionnement de ces blocs repose sur une logique d'alternance entre deux états principaux : l'approche de la pièce et le retour à la position de départ. Lorsqu'une pièce est détectée par la caméra, le robot reçoit la position cible et commence à s'approcher de la pièce en suivant la trajectoire calculée. Si, à tout moment, la pièce n'est plus détectée (par exemple, si elle a été retirée), le robot interrompt son mouvement et retourne automatiquement à sa position de départ pour attendre une nouvelle détection.

Cette logique permet d'assurer la sécurité et la robustesse du système: le robot ne tente jamais de saisir une pièce absente et se remet toujours dans une position sûre en cas d'erreur ou d'absence de détection. De plus, cela facilite la reprise du processus dès qu'une nouvelle pièce est détectée, sans intervention manuelle.

L'implémentation de cette alternance est réalisée à l'aide du bloc de détection de la pièce, qui active ou désactive les blocs en fonction de la confirmation de la présence de la pièce.


\subsection{Blocs de vérification et d'avortement}

Les blocs de vérification et d'avortement sont des éléments de sécurité ajoutés afin de s'assurer que le robot ne continue pas sa séquence si la pièce n'est pas prise. Cela rajoute un point de sécurité qui minimise donc les conséquences d'une mauvaise préhension de la pièce.  Cette décision a été prise suite à l'élaboration du tableau des risques du \textbf{chapitre \ref{chap:analyse}}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/figures/AICA_abort_sequence.png}
    \caption{Blocs de vérification et d'avortement dans AICA Studio}
    \label{fig:verification_block}
\end{figure}

On peut voir une interaction directe entre le bloc de séquence principal et le bloc d'avortement. Cela est dû à la nécessité d'arrêter la séquence principale si jamais la pièce n'est pas dans les pinces du robot au moment de la vérification. En amont, un bloc logique \textbf{ET} permet de prendre le signal de détection de pièce et le signal de validation de position venant du bloc du robot pour faire la vérification. Le robot reste en attente de validation pendant 5 secondes. Si la validation n'est pas faite, le robot retourne en position de départ et la séquence est avortée. Si la validation est faite, le robot continue sa séquence normalement.

\subsection{Programmation des mouvements}

Pour commander les mouvements du robot dans le bloc de séquence, il suffit de lui transmettre un \gls{payload} au format YAML décrivant la trajectoire souhaitée. Par exemple, on peut spécifier les positions articulaires à atteindre et les instants correspondants sous la forme suivante :
\texttt{{joint\_positions: [jreposition\_1, jrepositionnement, jreposition\_5], times\_from\_start: [1, 2.5, 5]}}.
Le bloc du robot interprète alors ce message et exécute la séquence de mouvements définie, en amenant le bras successivement à chaque position à l’instant indiqué.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/figures/AICA_payload.png}
    \caption{Bloc de programmation des mouvements dans AICA Studio}
    \label{fig:payload_block}
\end{figure}

Dans ce cas, la liste des mouvements est donnée en premier sous forme de positions articulaires. Ensuite, les chiffres correspondent au moment ou chaque position doit être atteinte. Cette approche permet de contrôler précisément le temps total de la séquence mais rend toute modification compliquée. En effet, si l'on souhaite modifier la durée d'un mouvement, il faut recalculer tous les mouvements suivants pour que le robot arrive à la bonne position au bon moment. Cela peut être un inconvénient si l'on souhaite faire des modifications fréquentes ou pendant la phase de test. Néanmoins, dans une mise à jour récente, la possibilité de modifier la durée de chaque mouvement indépendament des autres a été ajoutée.

Il est aussi possible de donner des points dans l'espace avec une orientation plutôt qu'une configuration articulaire fixe. Cette possibilité est plus simple à mettre en place mais est utile pour des positions et mouvements moins exigeants que ceux programmés dans le projet. Le bras robot possédant deux axes pouvants tourner à plus que 360 degrés, il se retrouvais régulièrement dans des situations ou les axes étaients proches des limites de leur rotation. Cela rendait le robot imprévisible et menait à des erreurs aléatoires qui bloquaient le système.
